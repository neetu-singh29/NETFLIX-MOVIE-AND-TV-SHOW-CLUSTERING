{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    -Netflix Movies and TV Shows Clustering Project Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name**            - Neetu Singh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "* **Netflix Movies and TV-shows Segmentation is the the unsupervised Machine Learning problem. In this problem by goal is to first make the usable for analysing and clustering process.**\n",
        "\n",
        "* The first step I did after **importing the dataset** is knowing the basic information about the dataset. After that I planned the **Data Wrangling** part and **Data preprocessing** steps. First I converted a date feature is by default loaded as the object to pandas datetime object so that I can use that feature easily for future purpose as datetime object gives many predefined methods to specifically work with date.\n",
        "\n",
        "* Next I didn't **handled the missing values** in the dataset, because the missing data in appeared in some features with less in numbers so we can use that for EDA which doesn't make any difference. Next thing is I going to use the text based feature that is \"Description\" for training machine learning model. These are the reason behind of not handling the missing values.\n",
        "\n",
        "* The next step is text preprocessing, before making this feature into meaningful multi-dimensional vector, I need to do some preprocessing steps that is **changing to lowercase, then removed punctual, stopping words and extra white spaces**, etc..\n",
        "\n",
        "* Finally,\n",
        "I used the **TfidVector** to **change the text data into  numerical**,I chose 400 as the maximum features so each review observation will be converted into 400 length features.\n",
        "\n",
        "* Then after converting this vector the next step is building clustering machine learning models. I tried three different models, the first one is **K-means**, then I tried **Hierarchical Clustering** and finally **DBSCAN algorithm**.\n",
        "---\n",
        "**Summary:** Overall summary of this project is:\n",
        "\n",
        "* This project focuses on clustering Netflix movies and TV shows based on their descriptions using unsupervised machine learning techniques. The key steps included:\n",
        "\n",
        " 1. Data wrangling and preprocessing\n",
        "\n",
        " 2. Exploratory Data Analysis (EDA)\n",
        "\n",
        " 3. Text preprocessing and vectorization\n",
        "\n",
        " 4. Implementation of three clustering models (K-Means, Hierarchical, DBSCAN)\n",
        "\n",
        " 5. Evaluation and selection of the best performing model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "**Link:** https://github.com/neetu-singh29/NETFLIX-MOVIE-AND-TV-SHOW-CLUSTERING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "* This dataset consists of tv shows and movies available on Netflix as of 2019.\n",
        "* The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "We need to do:\n",
        "\n",
        " * Exploratory Data Analysis\n",
        "\n",
        " * Understanding what type content is available in different countries\n",
        "\n",
        " * Is Netflix has increasingly focusing on TV rather than movies in recent years.\n",
        "\n",
        " * Clustering similar content by matching text-based features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install contractions"
      ],
      "metadata": {
        "id": "SnrzvjIN3l_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "# Import Libraries\n",
        "# Import Libraries# Import Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "\n",
        "#text preprocessing libraries\n",
        "import contractions\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.base import BaseEstimator,TransformerMixin\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "\n",
        "#avoid warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "\n",
        "#loading data\n",
        "df = pd.read_csv('NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIr7cYp42gRm"
      },
      "outputs": [],
      "source": [
        "df_copy = df.copy(deep = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Total Rows: \", df.shape[0])\n",
        "print(\"Total Columns: \", df.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated(keep = 'first').sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "\n",
        "df.isnull().sum().plot(kind= 'bar')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel(\"Total Number of NaN values\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "**Answer :** This is a Unsupervised machine project which means we don't have any target variable. This dataset contains 12 features and 7787 observations. Initially it had some duplicated observations and I removed that.\n",
        "This dataset contains movies and series details. And it has some categorical, Text (Description), and Numbers too. Finally there is four features with NaN values director, cast, country and date_added"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn3emhoA2gRn"
      },
      "outputs": [],
      "source": [
        "#Categorical Dataset Describe\n",
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "1. **show_id :** Unique ID and it is not useful much.\n",
        "2. **type :** This describes whether the particular observation is series or a movie\n",
        "3. **title :** This is the title of the movie or show\n",
        "4. **director :** Name of the director\n",
        "5. **cast :** cast information\n",
        "6. **country:** Name of the country where the movie/series are belongs to.\n",
        "7. **date_added :** Date is added in Netflix\n",
        "8. **release_year :** the date it was actually released.\n",
        "9. **rating :** Tv rating of the show\n",
        "10. **duration:** Duration of the movie (in minutes) or TV show (number of seasons).\n",
        "11. **listed_in :** Categories/genres the show belongs to (e.g., Dramas, Comedies).\n",
        "12. **description :** A brief description or summary of the content.\n",
        ".....\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "#Calculaing unique values for most continous cols might not give any insight. Sometime I may want to display\n",
        "#unique values of only catgorical/object dtypes so I created function here.\n",
        "\n",
        "def unique_counts(df1):\n",
        "   for i in df1.columns:\n",
        "       count = df1[i].nunique()\n",
        "       print(i, \": \", count)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoPkpVB52gRo"
      },
      "outputs": [],
      "source": [
        "#printing unique value of only object cols\n",
        "unique_counts(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rGGMQHX2gRo"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uz3eTC02gRp"
      },
      "outputs": [],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "#changing date_added feature into pandas datetime\n",
        "\n",
        "def handle_date_added_feature(date_added_values):\n",
        "    fin_date = []\n",
        "    for date in date_added_values:\n",
        "        if pd.isna(date):\n",
        "            fin_date.append(np.nan)\n",
        "        else:\n",
        "            #extracting day\n",
        "            day = date.split()[1]\n",
        "            day = int(day[:-1])\n",
        "            #extracting month\n",
        "            month = date.split()[0]\n",
        "            month_map = {'January':1,'February':2,'March':3,'April':4,'May':5,'June':6,'July':7,'August':8,'September':9,'October':10,'November':11,'December':12}\n",
        "            month =  month_map[month]\n",
        "            #extracting year\n",
        "            year = date.split()[-1]\n",
        "            fin_date.append(f'{year}-{month}-{day}')\n",
        "    #returning as datetime\n",
        "    return pd.to_datetime(fin_date)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRvquVGH2gRp"
      },
      "outputs": [],
      "source": [
        "df['date_added'] =  handle_date_added_feature(df.date_added)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWPnQ8Ee2gRp"
      },
      "outputs": [],
      "source": [
        "df['cast'] = df['cast'].apply(lambda x : np.nan if pd.isna(x) else x.split(','))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "**Answer:** This is unsupervised problem, so we going to use most of the features for EDA and our goal is to cluster based on the text features and I not going to spend much time in imputing Missing values and this wrangling I converted the date_added feature to pandas dataframe to use that feature effectively. Finally I changed the listed_in and cast to list so that we can use that in EDA little more easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1: Top Directors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code\n",
        "tv_show = df[df.type == 'TV Show']\n",
        "movie = df[df.type == 'Movie']\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "tv_show.director.value_counts()[:15].plot(kind='bar',ax = ax1,title='Top 15 TV Show Directors',figsize = (20,8))\n",
        "movie.director.value_counts()[:15].plot(kind='bar',ax =ax2, title = 'Top 15 Movie Directors',figsize = (20,8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "**Answer Here :** I chose bar chart with subplot option in python. I chose bar chart because it helps to understand the data with categorical and numberical feature. Here our categorical data is directors name and on y-axis it is count of their their movies/Tvshows in our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "**Answer:** Alastair Fothergill directed most TV shows in our dataset with total count of 3 TV Shows. Raul Campos is the directed most films in our Movie category with the total movie count of 18."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "**Answer:** These directors must be talented directors as the producers, there should be more fan base for these directors so Netflix can tieup with them to do a project only for Netflix, so that Netflix can make more money from that and their is no need to pay for the rights too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 2 :Top Actors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code\n",
        "tv_show_cst = []\n",
        "for obs in tv_show.iterrows():\n",
        "    if type(obs[1]['cast']) is list:\n",
        "        tv_show_cst.extend(obs[1]['cast'])\n",
        "\n",
        "movie_cst = []\n",
        "for obs in movie.iterrows():\n",
        "    if type(obs[1]['cast']) is list:\n",
        "        movie_cst.extend(obs[1]['cast'])\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "pd.Series(tv_show_cst).value_counts()[:15].plot(kind='bar',ax = ax1,title='Top 15 TV Show Actors',figsize = (20,8))\n",
        "pd.Series(movie_cst).value_counts()[:15].plot(kind='bar',ax =ax2, title = 'Top 15 Movie Actors',figsize = (20,8))\n",
        "plt.show()\n",
        "\n",
        "del tv_show_cst,movie_cst\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "**Answer Here:** I chose bar chart with subplot option in python. I chose bar chart because it helps to understand the data with categorical and numberical feature. Here our categorical data is actors name and on y-axis it is count of their movies/Tvshows in our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "**Answer:** Takahiro Sakurai acted most TV shows in our dataset with total count of 25 TV Shows. Anupam Kher is the acted most films in our Movie category with the total movie count of 32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "**Answer:** No business insights from the above chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3: Duration Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "#Checking the distribution of Movie Durations\n",
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "sns.distplot(movie['duration'].str.extract('(\\d+)'),kde=False)\n",
        "plt.title('Distplot with Normal distribution for Movies',fontweight=\"bold\")\n",
        "\n",
        "#Checking the distribution of TV SHOWS\n",
        "plt.subplot(2,1,2)\n",
        "plt.title(\"Distribution of TV Shows duration\",fontweight='bold')\n",
        "sns.countplot(x=tv_show['duration'],data=tv_show,order = tv_show['duration'].value_counts().index)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "**Answer:** I used histogram to find the distribution between movie and its duration and I used bar-graph to find the distribution between Tv_show and its duration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "**Answer:** Most TV shows in our dataset is released in only one season almost 1600 and Most films released, has 800 minutes duration and it is normaly distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4: Content by Country (Word Cloud)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# Chart - 4 visualization code\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "#Checking the distribution of TV SHOWS\n",
        "df_word_cloud = tv_show['country'].dropna()\n",
        "text = \" \".join([word.strip().replace(' ','_') for word in df_word_cloud])\n",
        "wordcloud = WordCloud(background_color=\"black\").generate(text)\n",
        "# Display the generated image:\n",
        "# the matplotlib way:\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.imshow(wordcloud, interpolation='blackman')\n",
        "plt.axis(\"off\")\n",
        "plt.title('TV_Show Country', fontsize=18, fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZxZLsr42gRr"
      },
      "outputs": [],
      "source": [
        "df_word_cloud = movie['country'].dropna()\n",
        "text = \" \".join([word.strip().replace(' ','_') for word in df_word_cloud])\n",
        "wordcloud = WordCloud(background_color=\"black\").generate(text)\n",
        "# Display the generated image:\n",
        "# the matplotlib way:\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.imshow(wordcloud, interpolation='blackman')\n",
        "plt.axis(\"off\")\n",
        "plt.title('Movie Country', fontsize=18, fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "**Answer:** It is the fancy way to find the most number of repeated data/string. It differentiate category by its frequency using  size of font which represents that category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "**Answer:** Netflix has more number of TV_shows and Movies belongs to United States, we could guess this easily as Netflix belongs to America so It would be their first market preference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "**Answer:** Having more movies and TV_show from United States is actually good for Netflix because as it from United States it should be an English content, and we know that many contry's people know English as their secondary languaage, so it would be become biggest profit for Netflix has they can make the same content available for other countries. So without making any addtional efforts they can make money from other country citizens too because of the language and the quality of the content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 5: Top Genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# Top 10 Genre in movies\n",
        "plt.figure(figsize=(18,6))\n",
        "sns.barplot(x = df[\"listed_in\"].value_counts().head(15).index,\n",
        "            y = df[\"listed_in\"].value_counts().head(15).values)\n",
        "plt.xticks(rotation=80)\n",
        "plt.title(\"Top10 Genre in Movies\",size='16',fontweight=\"bold\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "**Answer Here:** I chose bar chart with subplot option in python. I chose bar chart because it helps to understand the data with categorical and numberical feature. Here our categorical data is Genre and on y-axis it is count of movies/Tv Shows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "**Answer:** The Documentaries, Stand_up comedy and Dramas,International Movies is very famous and most contents available in Netflix has same genre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6 : Yearly Production Growth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# visualizing the movies and tv_shows based on the release year\n",
        "\n",
        "movies_year =movie['release_year'].value_counts().sort_index(ascending=False)\n",
        "tvshows_year =tv_show['release_year'].value_counts().sort_index(ascending=False)\n",
        "\n",
        "sns.set(font_scale=1.4)\n",
        "movies_year.plot(figsize=(12, 8), linewidth=2.5, label=\"Movies / year\")\n",
        "tvshows_year.plot(figsize=(12, 8), linewidth=2.5,label=\"TV Shows / year\")\n",
        "plt.xlabel(\"Years\", labelpad=15)\n",
        "plt.ylabel(\"Number\", labelpad=15)\n",
        "plt.title(\"Production growth yearly\", y=1.02, fontsize=22)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "**Answer:**line plot helps us to identify and see the trend clearly especially over time. My aim is to know the count of total number of contents became available  over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "**Answer:** As we can see that Netflix acheived its peak between 2017 to 2020. This may because of Corona. Because of corona people stayed in their house which make them to spend more time in internet.So it is clearly understood by Netflix and they make sure to own more contents in that period to attract more subscribers over other OTT platform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIx-8_IphqN"
      },
      "source": [
        "#### Chart - 7 : TV Show Ratings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 7 visualization code\n",
        "#Rating based on rating system of all TV Shows\n",
        "\n",
        "tv_ratings = tv_show.groupby(['rating'])['show_id'].count().reset_index(name='count').sort_values(by='count',ascending=False)\n",
        "fig_dims = (14,7)\n",
        "fig, ax = plt.subplots(figsize=fig_dims)\n",
        "sns.pointplot(x='rating',y='count',data=tv_ratings)\n",
        "plt.title('TV Show Ratings',size='20')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27r6nlMphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv6ro40sphqO"
      },
      "source": [
        "**Answer:** point plot helps us to find the trend among different category. Here in this we are counting the total of different movies/tv shows ratings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jJGEOYphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6ZPi4hphqO"
      },
      "source": [
        "**Answer:** TV-MA rating category content is more available in Netflix and followed by TV-14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 8 :Content by Country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "## Chart - 8 visualization code\n",
        "# Count of TVshow and Movie produced in different country\n",
        "\n",
        "\n",
        "df_country = df.groupby(['country', 'type'])['show_id'].count().sort_values(ascending = False).reset_index()\n",
        "plt.figure(figsize = (15, 6))\n",
        "sns.barplot(data = df_country, x = df_country['country'][:20], y = df_country['show_id'], hue = 'type')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.ylabel('Total Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "**Answer:** I chose bar chart with subplot option in python. I chose bar chart because it helps to understand the data with categorical and numberical feature. Here our categorical data is Country and on y-axis it is count of TV shows and movies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtgC_hjphqO"
      },
      "source": [
        "**Answer:** As we seen in the word cloud Netflix has more number of United States's Tv shows and Movies. Followed by India, which has highest number of movies and very low number of TV_shows comparing to the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "**Hypothesis 1: Netflix content comparison**\n",
        "\n",
        "**HO (Null Hypothesis):** Netflix has greater or equal to number of movies collectively.(Netflix has â‰¥ movies than TV shows)\n",
        "\n",
        "**H1 (Alternate Hypothesis):** Netflix has always less number of movies than TV-show collectively.(Netflix has < movies than TV shows)\n",
        "\n",
        "---\n",
        "**Test:** Two-sample t-test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1mmZmTf2gRu"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "import numpy as np\n",
        "\n",
        "movie_grp = df.groupby(['type',df.date_added.dt.year])['show_id'].count()['Movie']\n",
        "tv_show = df.groupby(['type',df.date_added.dt.year])['show_id'].count()['TV Show']\n",
        "\n",
        "movie_grp_mean = np.mean(movie_grp)\n",
        "tv_show_mean = np.mean(tv_show)\n",
        "\n",
        "print(\"Movie Group mean value:\",movie_grp_mean)\n",
        "print(\"TV_Show mean value:\",tv_show_mean)\n",
        "\n",
        "value_for_mean_std = np.std(movie_grp)\n",
        "overall_std = np.std(tv_show)\n",
        "\n",
        "print(\"\\n Movie std value:\",value_for_mean_std)\n",
        "print(\"TV_Show std value:\",overall_std)\n",
        "\n",
        "ttest,pval = ttest_ind(movie_grp,tv_show)\n",
        "\n",
        "print(\"\\n p-value\",pval)\n",
        "if pval < 0.05:\n",
        "  print(\"\\nðŸ§¿ we reject null hypothesis\")\n",
        "else:\n",
        "  print(\"\\nðŸ§¿ we accept null hypothesis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "**Answer:** I did **Two-sample T-test**.\n",
        "* A t-test is a type of inferential statistic which is used to determine if there is a significant difference between the means of two groups which may be related in certain features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "**Answer:** The One Sample t Test determines whether the sample mean is statistically different from a known or hypothesised population mean.\n",
        " The One Sample t Test is a parametric test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "\n",
        "**Hypothesis 2: Movie durations**\n",
        "\n",
        "**Null (H0):** movies rated for kids and older kids are at least two hours long.(Kids' movies are â‰¥ 2 hours long)\n",
        "\n",
        "**Alternate (H1):** movies rated for kids and older kids are not at least two hours long.(Kids' movies are < 2 hours long)\n",
        "\n",
        "---\n",
        "**Test:** One-sample t-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y3pqfN42gRu"
      },
      "outputs": [],
      "source": [
        "df_hypothesis = df.copy(deep=True)\n",
        "df_hypothesis['duration']= df_hypothesis['duration'].str.extract('(\\d+)')\n",
        "df_hypothesis['duration'] = pd.to_numeric(df_hypothesis['duration'])\n",
        "df_hypothesis['type'] = pd.Categorical(df_hypothesis['type'], categories=['Movie','TV Show'])\n",
        "\n",
        "#group_by duration and TYPE\n",
        "group_by_= df_hypothesis[['duration','type']].groupby(by='type')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "movie_grp = group_by_.get_group('Movie')\n",
        "tv_show = group_by_.get_group('TV Show')\n",
        "\n",
        "# Ensure 'duration' column is numeric\n",
        "movie_grp['duration'] = pd.to_numeric(movie_grp['duration'], errors='coerce')\n",
        "tv_show['duration'] = pd.to_numeric(tv_show['duration'], errors='coerce')\n",
        "\n",
        "movie_grp_mean = np.mean(movie_grp['duration']) # Calculate mean of 'duration'\n",
        "tv_show_mean = np.mean(tv_show['duration']) # Calculate mean of 'duration'\n",
        "\n",
        "print(\"Movie Group mean value:\",movie_grp_mean)\n",
        "print(\"TV_Show mean value:\",tv_show_mean)\n",
        "\n",
        "movie_grop_std = np.std(movie_grp['duration']) # Calculate std of 'duration'\n",
        "tv_show_std = np.std(tv_show['duration']) # Calculate std of 'duration'\n",
        "\n",
        "print(\"\\n Movie std value:\",movie_grop_std) # Use the correct variable\n",
        "print(\"TV_Show std value:\",tv_show_std) # Use the correct variable\n",
        "\n",
        "\n",
        "#length of groups and DOF\n",
        "n1 = len(movie_grp)\n",
        "n2= len(tv_show)\n",
        "\n",
        "\n",
        "dof = n1+n2-2\n",
        "\n",
        "sp_2 = ((n2-1)*movie_grop_std**2  + (n1-1)*tv_show_std**2) / dof\n",
        "\n",
        "sp = np.sqrt(sp_2)\n",
        "\n",
        "#tvalue\n",
        "t_val = (movie_grp_mean-tv_show_mean)/(sp * np.sqrt(1/n1 + 1/n2))\n",
        "print('\\n t_value:',t_val) # No need to index t_val\n",
        "\n",
        "\n",
        "if (stats.t.ppf(0.025,dof) < t_val) and (t_val < stats.t.ppf(0.975,dof)):\n",
        "    print(\"\\nðŸ§¿ We Accept the Null Hypothesis\")\n",
        "else :\n",
        "    print(\"\\nðŸ§¿ We reject the Null Hypothesis\")"
      ],
      "metadata": {
        "id": "XLGaqrdL5XU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "**Answer:** I did Two-sample T-test.A t-test is a type of inferential statistic which is used to determine if there is a significant difference between the means of two groups which may be related in certain features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        "**Answer:** The One Sample t Test determines whether the sample mean is statistically different from a known or hypothesised population mean.\n",
        " The One Sample t Test is a parametric test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "The Description is the only features which I need for training ML model, and I don't have any missing values in the Description feature so there is no need to handle impute any missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "There is no outliers as I am going use only Description Feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "I going to handle only text data here so, I am not going to taking care of other variables here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwf50b-R2tYG"
      },
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n",
        "---\n",
        "**Text Processing Steps:**\n",
        "\n",
        "* Expanded contractions\n",
        "\n",
        "* Converted to lowercase\n",
        "\n",
        "* Removed punctuation\n",
        "\n",
        "* Removed URLs and digits\n",
        "\n",
        "* Removed stopwords\n",
        "\n",
        "* Lemmatization (chosen over stemming to preserve context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQiZwjn3iu7"
      },
      "source": [
        "#### 1. Expand Contraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey-3AIQy2gRw"
      },
      "outputs": [],
      "source": [
        "#decoding non-utf-8 characters\n",
        "def remove_non_utf8_words(df,features_names):\n",
        "    df = df.copy()\n",
        "    for feature in features_names:\n",
        "        df[feature] = df[feature].apply(lambda x : x.replace('Ã¢â‚¬â„¢',\"'\"))\n",
        "        df[feature] = df[feature].apply(lambda x : ''.join([c for c in x if ord(c) < 128]))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36WVmM9R2gRw"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction\n",
        "def expand_contractions(df,feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x : \" \".join(x.split()))\n",
        "        df[feature] = df[feature].apply(lambda x : contractions.fix(x))\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIkgGqN3qsr"
      },
      "source": [
        "#### 2. Lower Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flD1NELp2gRw"
      },
      "outputs": [],
      "source": [
        "# Lower Casing\n",
        "def change_to_lower_case(df,feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x : x.lower())\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPnILGE3zoT"
      },
      "source": [
        "#### 3. Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eki2SZnW2gRw"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuations(df,features_columns):\n",
        "    df = df.copy()\n",
        "    punctuations = string.punctuation\n",
        "    for feature in features_columns:\n",
        "        df[feature] = df[feature].apply(lambda x : x.translate(str.maketrans('','',punctuations)))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlsf0x5436Go"
      },
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "outputs": [],
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_urls(df,feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x :  re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x))\n",
        "    return df\n",
        "\n",
        "def remove_words_with_digits(df, feature_names):\n",
        "    df = df.copy()\n",
        "    for feature in feature_names:\n",
        "        df[feature] = df[feature].apply(lambda x : \" \".join(s for s in x.split() if not any(c.isdigit() for c in s)))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9DMSJo4nBL"
      },
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords\n",
        "def remove_stopwords(df,features_names):\n",
        "    df = df.copy()\n",
        "    eng_stopwords = set(stopwords.words('english'))\n",
        "    for feature in features_names:\n",
        "        df[feature] = df[feature].apply(lambda text: \" \".join(word for word in text.split() if not word in eng_stopwords))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRK590QV2gRx"
      },
      "source": [
        "#### 6. Tokenization and Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vMIqE192gRx"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "def tokenize_and_normalization(df,feature_names):\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x :\" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj1E5IA-2gRx"
      },
      "source": [
        "##### Which text normalization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKZzH__E2gRx"
      },
      "source": [
        "**Answer:** I using Lemmatization normalization because it will do better than stemming. In stemming, there is chance that it will change the word completely. But in case of Lemmatization, it is not the case, it will try to maintain the original context of the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfrHMr5s2gRx"
      },
      "source": [
        "#### 7. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2tSJCsv2gRx"
      },
      "outputs": [],
      "source": [
        "class CustomTextDataPreprocessing(BaseEstimator,TransformerMixin):\n",
        "\n",
        "    def __init__(self,feature_names):\n",
        "        self.feature_names = feature_names\n",
        "        return None\n",
        "\n",
        "    #decoding non-utf-8 characters\n",
        "    def remove_non_utf8_words(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : x.replace('Ã¢â‚¬â„¢',\"'\"))\n",
        "            df[feature] = df[feature].apply(lambda x : ''.join([c for c in x if ord(c) < 128]))\n",
        "        return df\n",
        "\n",
        "    # Expand Contraction\n",
        "    def expand_contractions(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : \" \".join(x.split()))\n",
        "            df[feature] = df[feature].apply(lambda x : contractions.fix(x))\n",
        "        return df\n",
        "\n",
        "        # Lower Casing\n",
        "    def change_to_lower_case(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : x.lower())\n",
        "        return df\n",
        "\n",
        "        # Remove Punctuations\n",
        "    def remove_punctuations(self,df,features_names):\n",
        "        df = df.copy()\n",
        "        punctuations = string.punctuation\n",
        "        for feature in features_names:\n",
        "            df[feature] = df[feature].apply(lambda x : x.translate(str.maketrans('','',punctuations)))\n",
        "        return df\n",
        "\n",
        "    # Remove URLs & Remove words and digits contain digits\n",
        "    def remove_urls(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x :  re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x))\n",
        "        return df\n",
        "\n",
        "    def remove_words_with_digits(self,df, feature_names):\n",
        "        df = df.copy()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x : \" \".join(s for s in x.split() if not any(c.isdigit() for c in s)))\n",
        "        return df\n",
        "\n",
        "    # Remove Stopwords\n",
        "    def remove_stopwords(self,df,feature_names):\n",
        "        df = df.copy()\n",
        "        eng_stopwords = set(stopwords.words('english'))\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda text: \" \".join(word for word in text.split() if not word in eng_stopwords))\n",
        "        return df\n",
        "\n",
        "    # Tokenization\n",
        "    def tokenize_and_normalization(self,df,feature_names):\n",
        "        lemmatizer=WordNetLemmatizer()\n",
        "        for feature in feature_names:\n",
        "            df[feature] = df[feature].apply(lambda x :\" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "\n",
        "    def transform(self,df):\n",
        "        df =df.copy()\n",
        "        #removing non utf8 words\n",
        "        df = self.remove_non_utf8_words(df,self.feature_names)\n",
        "        #expanding contractions\n",
        "        df = self.expand_contractions(df,self.feature_names)\n",
        "        #changing all to lower case\n",
        "        df = self.change_to_lower_case(df,self.feature_names)\n",
        "        #remvoing punctuations\n",
        "        df = self.remove_punctuations(df,self.feature_names)\n",
        "        #removing urls\n",
        "        df = self.remove_urls(df,self.feature_names)\n",
        "        #removing words with digits\n",
        "        df = self.remove_words_with_digits(df,self.feature_names)\n",
        "        #remove stopwords\n",
        "        df = self.remove_stopwords(df,self.feature_names)\n",
        "        #remove tokenize and normalization\n",
        "        df = self.tokenize_and_normalization(df,self.feature_names)\n",
        "\n",
        "\n",
        "        return df\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbSTc_jI2gRx"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text\n",
        "class CustomTfidVectorizer(BaseEstimator,TransformerMixin):\n",
        "    def __init__(self,feature_name,max_features = None):\n",
        "        self.max_features = max_features\n",
        "        self.feature_name = feature_name\n",
        "        return None\n",
        "\n",
        "    def fit(self,df):\n",
        "        self.TfidVectorizer = TfidfVectorizer(max_features= self.max_features)\n",
        "        self.TfidVectorizer.fit(df[self.feature_name])\n",
        "        return self\n",
        "\n",
        "    def transform(self,df):\n",
        "        df = df.copy()\n",
        "        vectors = self.TfidVectorizer.transform(df[self.feature_name]).toarray()\n",
        "        #df[self.TfidVectorizer.get_feature_names()] = vectors\n",
        "        df[self.TfidVectorizer.get_feature_names_out()] = vectors\n",
        "        df.drop(self.feature_name,axis = 1,inplace = True)\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhazaKne2gRx"
      },
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhJuShQ62gRx"
      },
      "source": [
        "**Answer:** Used TF-IDF (400 max features) as it considers word importance relative to corpus.\n",
        "\n",
        " * Term frequency-inverse document frequency ( TF-IDF) gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document and a corpus.Not like Bag of words and Count Vector technique which treats all words equally,TF-IDF can distinguish very common words or rare words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 5. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "outputs": [],
      "source": [
        "# Select your features wisely to avoid overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMng2IbBLp7"
      },
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      },
      "source": [
        "I am going to use only Description feature so I am going to drop rest of the columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      },
      "source": [
        "##### Which all features you found important and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgaEstsBnaf"
      },
      "source": [
        "I am going to use all the features got from vectorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 6. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "outputs": [],
      "source": [
        "# Transform Your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 7. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "outputs": [],
      "source": [
        "# Scaling your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehVS_eP72gRy"
      },
      "source": [
        "There is no need to scale the data I've finally got, I've checked the data, the vectorization got is actually already in normalized form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 8. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "outputs": [],
      "source": [
        "# DImensionality Reduction (If needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5CmagL3EC8N"
      },
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKr75IDuEM7t"
      },
      "source": [
        "I am going to use all the features which I got after vectorization to maintain the information. Dimensionality Reduction doing in the vectorization may loose its meaning of every vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 9. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "outputs": [],
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      },
      "source": [
        "No need to split the data as it is unsupervised problem, we cannot test the result of model by having the test data. so, I am going to use the whole dataset for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 10. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "outputs": [],
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfAFQswW2gRz"
      },
      "outputs": [],
      "source": [
        "#using pipeline to transform our text data\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text_feature_pipeline = Pipeline([\n",
        "    ('text_preprocessing',CustomTextDataPreprocessing(feature_names=['description'])),\n",
        "    ('vectorization',CustomTfidVectorizer(feature_name='description',max_features=400))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b7SvnKo2gR0"
      },
      "outputs": [],
      "source": [
        "description_feature_vector = text_feature_pipeline.fit_transform(df).iloc[:,11:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgabVwiY2gR0"
      },
      "outputs": [],
      "source": [
        "#printing the shapes of our data\n",
        "print(\"Train data: \",description_feature_vector.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1: K-Means**\n",
        "Optimal clusters: 12 (from elbow method)"
      ],
      "metadata": {
        "id": "jAjNDPuM71-w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zY-U1Xwb2gR0"
      },
      "outputs": [],
      "source": [
        "SEED = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "#finding optimal number of clusters using the elbow method\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "ssd= []\n",
        "\n",
        "#Using for loop for iterations from 1 to 30.\n",
        "for cluster in range(2, 15):\n",
        "    kmeans = KMeans(n_clusters=cluster,random_state= SEED)\n",
        "    kmeans.fit(description_feature_vector)\n",
        "    preds = kmeans.predict(description_feature_vector)\n",
        "    score = silhouette_score(description_feature_vector, preds)\n",
        "    print(\"For n_clusters = {}, Silhouette score is {}\".format(cluster, score))\n",
        "    ssd.append(kmeans.inertia_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.plot(range(2, 15), ssd)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Sum of Squared distance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBhVsr7c2gR0"
      },
      "source": [
        "From the above Elbow method and silhouette score I am choosing 12 would be the perfect number of clusters for this problem. So I am finally training kmeans with 12 clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maF3rPJ72gR0"
      },
      "outputs": [],
      "source": [
        "#training the K-means model on a dataset\n",
        "kmeans = KMeans(n_clusters= 12, init='k-means++', random_state= SEED)\n",
        "y_predict= kmeans.fit_predict(description_feature_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oQNtQNF2gR0"
      },
      "outputs": [],
      "source": [
        "#Predict the clusters and evaluate the silhouette score\n",
        "score = silhouette_score(description_feature_vector, y_predict)\n",
        "print(\"Silhouette score is {}\".format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCxGccWv2gR0"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Reducing the number of features to visualize it in 2D or 3D plot\n",
        "pca = PCA(n_components = 3)\n",
        "X = pca.fit_transform(description_feature_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmS_c-Xw2gR0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.title('customer segmentation based on Recency and Monetary')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_predict, s=50)\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQnZ3tAx2gR1"
      },
      "outputs": [],
      "source": [
        "#plotting 3D Graph\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig=plt.figure(figsize=(15,10))\n",
        "plt.title('3d visualization of Recency Frequency and Monetary')\n",
        "ax=fig.add_subplot(111,projection='3d')\n",
        "xs=X[:,0]\n",
        "ys=X[:,1]\n",
        "zs=X[:,2]\n",
        "ax.scatter(xs,ys,zs,s=5,c = y_predict)\n",
        "ax.set_xlabel('PCA 0')\n",
        "ax.set_ylabel('PCA 1')\n",
        "ax.set_zlabel('PCA 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxGg7wVi2gR1"
      },
      "source": [
        "At the end, from the 2D and 3D plot we can say that K-means done not a bad job, but we cannot get into a conclusion by plotting this PCA features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fze-IPXLpx6K"
      },
      "source": [
        "### ML Model - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2: Hierarchical**\n",
        "\n",
        "Optimal clusters: 9 (from dendrogram),AgglomerativeClustering\n"
      ],
      "metadata": {
        "id": "vUe4Ck7G9Jff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKnBiSTg2gR1"
      },
      "outputs": [],
      "source": [
        "# Using the dendogram to find the optimal number of clusters\n",
        "import scipy.cluster.hierarchy as sch\n",
        "plt.figure(figsize=(13,8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(description_feature_vector, method = 'ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Customers')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show() # find largest vertical distance we can make without crossing any other horizontal line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "aggh = AgglomerativeClustering(n_clusters= 9, linkage='ward') #, affinity='euclidean'\n",
        "aggh.fit(description_feature_vector)\n",
        "#Predicting using our model\n",
        "y_hc=aggh.fit_predict(description_feature_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7Bv7FPu2gR1"
      },
      "outputs": [],
      "source": [
        "#Silhouette Coefficient\n",
        "print(\"Silhouette Coefficient: %0.3f\"%silhouette_score(description_feature_vector,y_hc, metric='euclidean'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRr8B_I02gR1"
      },
      "source": [
        "### ML Model - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 3: DBSCAN(Density-Based Spatial Clustering of Application with Noice)**"
      ],
      "metadata": {
        "id": "ANgOMmFd-OwS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5ug7BpW2gR1"
      },
      "outputs": [],
      "source": [
        "#model 3\n",
        "\n",
        "#finding optimal number of clusters using the elbow method\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "eps_range=range(6,12) # note, we will scale this down by 100 as we want to explore 0.06 - 0.11 range\n",
        "minpts_range=range(5,14)\n",
        "\n",
        "silhouette_scores= []\n",
        "comb = []\n",
        "\n",
        "#Using for loop for iterations from 1 to 30.\n",
        "for k in eps_range:\n",
        "    for j in minpts_range:\n",
        "        # Set the model and its parameters+\n",
        "        model = DBSCAN(eps=k/100, min_samples=j)\n",
        "        # Fit the model\n",
        "        clm = model.fit(description_feature_vector)\n",
        "        # Calculate Silhoutte Score and append to a list\n",
        "        silhouette_scores.append(silhouette_score(description_feature_vector, clm.labels_, metric='euclidean'))\n",
        "        comb.append(str(k)+\"|\"+str(j)) # axis values for the graph\n",
        "        print(str(k)+\"|\"+str(j))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('silhouette scores:',silhouette_scores)"
      ],
      "metadata": {
        "id": "5aow_H_lekfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDFkrx6o2gR1"
      },
      "outputs": [],
      "source": [
        "# Plot the resulting Silhouette scores on a graph\n",
        "plt.figure(figsize=(45,10))\n",
        "plt.plot(comb, silhouette_scores, 'bo-')\n",
        "plt.xlabel('Epsilon/100 | MinPts')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score based on different combnation of Hyperparameters')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AydoGk-F2gR1"
      },
      "source": [
        "Looking at the above, we can see that eps=0.08 produce the highest scores.\n",
        "A few combinations ended up having very similar scores, indicating that the clustering output for those combinations would also be similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjLoGEGG2gR1"
      },
      "outputs": [],
      "source": [
        "final_model = DBSCAN(eps=6/100, min_samples=5)\n",
        "clm = model.fit(description_feature_vector)\n",
        "print(silhouette_score(description_feature_vector, clm.labels_, metric='euclidean'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXEKarJi2gR1"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:,0], X[:,1], c= clm.labels_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "**Answer:** Silhouette score helped us to find the best model. And helped us to choose the best model among these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "**Answer:**  **K-Means** did better job in this dataset. It gives the insight that **`12 is the perfect number of cluster`** , but Incase of **Aggloramative** Clustering  we saw that the **`9 is the optimal cluster`**. Might be 9 is better, but I chose 12 in K-means, I got  a better result with that using k-means. And **DBSCAN** doen't perform well in this dataset.\n",
        "\n",
        "* **Chosen Model:** **K-Means** with 12 clusters\n",
        "\n",
        "* **Reason:** Better interpretability and slightly more meaningful clusters than hierarchical\n",
        "\n",
        "* **Evaluation Metric:** Silhouette score guided model selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "# Save the File\n",
        "\n",
        "filename = 'netflix_best_model.sav'\n",
        "joblib.dump(aggh, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      },
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gkQbRkD2gR2"
      },
      "outputs": [],
      "source": [
        "# Load the File and predict unseen data.\n",
        "loaded_model = joblib.load(filename)\n",
        "\n",
        "# Use 'fit_predict' on your data to get cluster assignments\n",
        "result = loaded_model.fit_predict(description_feature_vector)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "* We had given the problem of unsupervised clustering using Netflix Dataset. Initially we run some basic python code snippets to understand more about the data especially its shape, no.of features, datatypes, statical information etc.. Then in EDA part we analysed the data and found :\n",
        "\n",
        "\n",
        "1. Netflix has more number of TV_shows and Movies belongs to United States.\n",
        "\n",
        "2. Netflix acheived its peak in owning the number of contents between 2017 to 2020.\n",
        "\n",
        "3. Netflix has more number of United States's Tv shows and Movies. Followed by India, which has highest number of movies and very low number of TV_shows comparing to the others.\n",
        "\n",
        "\n",
        "**Clustering:**\n",
        "\n",
        "In K-means which did better, said 12 should be the better cluster.I case of Agglomerative Clustering which too did best job in the Silhoutte score and using the dendogram I found that 9 should be the perfect cluster for this model. But finally using the Silhoutte score I concluded that K-Means did very nice job with 12 clusters.\n",
        "\n",
        "---\n",
        "**Conclusion:** Overall conclution of this project:\n",
        "\n",
        "* The project successfully clustered Netflix content using textual descriptions. Key findings include:\n",
        "\n",
        " 1. US dominates Netflix content production\n",
        "\n",
        " 2. Documentaries and dramas are most common genres\n",
        "\n",
        " 3. Movies outnumber TV shows significantly\n",
        "\n",
        " 4. K-Means with 12 clusters provided the most meaningful grouping\n",
        "\n",
        "The model is ready for deployment to help Netflix with content organization and recommendation systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "35m5QtbWiB9F",
        "Yfr_Vlr8HBkt",
        "7wuGOrhz0itI",
        "578E2V7j08f6",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}